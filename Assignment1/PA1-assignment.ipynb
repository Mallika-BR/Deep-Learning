{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1\n",
    "## CSCI-5931 : Deep Learning\n",
    "## Spring 2022\n",
    "## Instructor: Ashis Kumer Biswas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c23fe1336d525080780d35d51206ef2d",
     "grade": false,
     "grade_id": "requirement_check",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b33d08a7305d47890e44a95f7aaa7d4",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Imports === #\n",
    "# Various python packages are used in this notebook. Please get yourself used to them (optional).\n",
    "import pandas as pd  # used for storing a tabular representation of the dataset, similar to XLS files.\n",
    "from pickle import dump  # used for saving class objects for access in another file\n",
    "from pickle import load  # used for loading class objects for access in another file\n",
    "from pathlib import Path # used to check if the saved model files and accessories.\n",
    "import requests #used to request remote judge.csv evaluation \n",
    "from sklearn.preprocessing import StandardScaler  # used for normalization of dataset\n",
    "from sklearn.model_selection import train_test_split  # used for performing the train-test split of a dataframe\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder   #My favorite categorical to numerical feature conversion tool\n",
    "from tensorflow import keras  # keras used for construction of the Artificial neural network\n",
    "\n",
    "import matplotlib.pyplot as plt  # used for training visualization\n",
    "import numpy as np  # numpy arrays used for matrix computations\n",
    "\n",
    "# === Extra Configurations for the GPU Environment === #\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(physical_devices)>0: #If you have at least one \"configured\" GPU, let's use it; otherwise, pass\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# === Random Seed Initialization === #\n",
    "random_seed = 54321  # will be passed into every function which accepts a random seed/state\n",
    "                     # so that, all submissions will produce the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11a132465d40bb3c31373cd6ff0a563e",
     "grade": false,
     "grade_id": "utility-csv-file-open",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# In order to begin preprocessing the dataset, we must first read it into memory. \n",
    "#The below function reads the dataset into a pandas dataframe and returns that dataframe unprocessed. \n",
    "#This dataframe will be used as input for future functions.\n",
    "\n",
    "def read_dataset( filepath ):\n",
    "    '''\n",
    "    input: \n",
    "       * filepath: string representation of path to file; mostly a csv file.\n",
    "    \n",
    "    returns: \n",
    "       * pandas dataframe containing the table present in \n",
    "         the dataset\n",
    "    \n",
    "     Task:\n",
    "       * call the pandas package's read_csv function on the \n",
    "         specified filepath and return the result.\n",
    "    '''\n",
    "    return pd.read_csv( filepath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : (5 points)\n",
    "write a function that taskes as argument the csv datafile, and a string (denoting a column name belonging to the dataset). The function must call the read_dataset() function above and returns as a list 5 items: total number of rows, total number of columns, minimum, maximum, mean, standard deviation of the given column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ef924ec7de41aa9dbbbefa6e98eb237",
     "grade": false,
     "grade_id": "reading",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lets_read_dataset(csv_file_name, column_name):\n",
    "    \"\"\"reads the csv file at csv_file_name, and particularly a column named column_name.\n",
    "    Then, it computes 5 values: total number of rows in the csv, total number of columns in the csv,\n",
    "    minimum, maximum, mean and standard deviation of values of the given column and\n",
    "    sets the 5 variables below accordingly. And finally returns these 5 as a list []\n",
    "    \"\"\"\n",
    "    total_number_of_rows = total_number_of_columns = minimum=maximum=mean=stdev = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8daca60ec0be53dfb0b0de97e6c2eab",
     "grade": false,
     "grade_id": "utility_function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sum_squared_difference(list1, list2):\n",
    "    \"\"\"Compute the sum of squared difference between elements from list1 and list2.\n",
    "    This function is heavily used in Autograder test validations.\n",
    "    \"\"\"\n",
    "    return np.sum( ( np.array(list1) - np.array(list2))**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27979b197e7b3edf1fcf1cd565b2a67a",
     "grade": true,
     "grade_id": "correct_reading",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that lets_read_dataset returns the correct output for several inputs\"\"\"\n",
    "\n",
    "assert  sum_squared_difference(lets_read_dataset('dataset/dataset.csv','CreditScore'),\n",
    "                              [9000, 13, 350, 850, 650.1172222222223, 96.64063404096387]) <= 0.1\n",
    "\n",
    "assert  sum_squared_difference(lets_read_dataset('dataset/dataset.csv','Age'),\n",
    "                              [9000, 13, 18, 92, 38.921, 10.488982743812672]) <= 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eabb64d10bc806104493e604687261b8",
     "grade": false,
     "grade_id": "final-reading-of-dataset",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Enough of reading dataset... Haha. Let's settle down for now. Print the first 10 samples  === #\n",
    "dataset = read_dataset( 'dataset/dataset.csv' )\n",
    "dataset.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (10 points)\n",
    "\n",
    "### First preprocessing that we are going to do is *Dropping selected Features (i.e., columns)*\n",
    "\n",
    "Now that we have the dataset in memory, we can drop features as desired. The dropping of features will be performed in a function defined in the following code cell. Features dropped are specified in the markdown cell at the beginning of the preprocessing section. This operation will be done functionally with a new dataframe being returned with the specified features dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "928cada634bb981a529878e90f40062c",
     "grade": false,
     "grade_id": "define_lets_drop_features",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lets_drop_features( data , features_to_drop ):\n",
    "    '''\n",
    "    input: \n",
    "       * data: pandas dataframe to have features dropped\n",
    "       * features_to_drop: list of columns (i.e., features) to be dropped.\n",
    "    \n",
    "    returns: \n",
    "       * pandas dataframe containing the original dataset without\n",
    "         the specified features\n",
    "    \n",
    "    Task:\n",
    "       * utilize pandas DataFrame.drop() function on the specified\n",
    "         features\n",
    "       * return the results as a new dataframe\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abb2174586581d85c1e0bc4a0341b723",
     "grade": false,
     "grade_id": "utility_to_check_drop",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_drop_features(csv_file,features_to_drop):\n",
    "    '''\n",
    "    An utility function to check your definition of lets_drop_features function.\n",
    "    Returns True if the definition is correct, otherwise it returns false.\n",
    "    '''\n",
    "    data = read_dataset( csv_file )\n",
    "    n_cols_before = len(data.columns)\n",
    "    data = lets_drop_features(data,features_to_drop)\n",
    "    n_cols_after = len(data.columns)\n",
    "    \n",
    "    so_far_so_good = True\n",
    "    for feature_name in features_to_drop:\n",
    "        if feature_name in data.columns:\n",
    "            so_far_so_good = False\n",
    "            break\n",
    "    if so_far_so_good == True:\n",
    "        if n_cols_before - n_cols_after != len(features_to_drop):\n",
    "            so_far_so_good = False\n",
    "    return so_far_so_good\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1093eb51c833261224d2e313624be58",
     "grade": true,
     "grade_id": "feature_drop_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that lets_drop_features returns the correct output for several inputs\"\"\"\n",
    "\n",
    "assert  check_drop_features('dataset/dataset.csv',['Age']) == True\n",
    "\n",
    "assert  check_drop_features('dataset/dataset.csv',['CustomerId' , 'Surname']) == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3c34b4c29f95a2aae4fc66a5be6b511",
     "grade": false,
     "grade_id": "cell-247a930d79bde8e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Let's settle down with the dropping of features: 'CustomerId' and 'Surname' === #\n",
    "features_to_drop = [ 'CustomerId' , 'Surname']\n",
    "dataset_dropped  = lets_drop_features( dataset , features_to_drop )\n",
    "dataset_dropped.head( n=10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Preprocessing that we are going to do is *Shuffle Rows*\n",
    "\n",
    "\"It is extremely important to shuffle the training data, so that you do not obtain entire minibatches of highly correlated examples. As long as the data has been shuffled, everything should work OK. Different random orderings will perform slightly differently from each other but this will be a small factor that does not matter much.\" -- [Ian Goodfellow](https://qr.ae/pGBgw8)\n",
    "\n",
    "To shuffle, we will write a wrapper function to call pandas built in DataFrame.sample() with a frac parameter of 1 (Return 100% of the dataset after shuffling) and a random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80923f8960f683d56b075dde25c1fd70",
     "grade": false,
     "grade_id": "cell-58d43aaebd19529d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_data( data , random_seed=12345 ):\n",
    "    '''\n",
    "    input: \n",
    "       * data: pandas dataframe to be shuffled\n",
    "       * random_seed (optional): seed for the random state\n",
    "            - Default = 12345\n",
    "    \n",
    "    returns: \n",
    "       * pandas dataframe containing a random shuffling of the\n",
    "         rows of the data parameter\n",
    "    \n",
    "    Task:\n",
    "       * use pandas DataFrame.sample() to select a random sample\n",
    "         of the data parameter. By default, the data is sampled\n",
    "         without replacement, so a frac=1 parameter will randomly\n",
    "         select 100% of the data entries distinctly (shuffling).\n",
    "         And using the random_seed parameter as the random_state\n",
    "         for the sample. \n",
    "    \n",
    "    Note:\n",
    "       - In order to exclude the use of a random state,\n",
    "         the random_seed parameter of this function must be\n",
    "         set to None type.\n",
    "    '''\n",
    "    return data.sample( frac=1 , random_state=random_seed )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "446f8549611ca772628d09462bb89b1c",
     "grade": false,
     "grade_id": "shuff_dataset_final",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Demonstrate Success of the Shuffle === #\n",
    "dataset_shuffled = shuffle_data( dataset_dropped , random_seed=random_seed )\n",
    "dataset_shuffled.head( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: (10 points)\n",
    "\n",
    "### Third Preprocessing that we will do is X-y Partitioning of the dataset\n",
    "\n",
    "In its current state, the dataset contains both independent (input) and the target features within the same dataframe. In order to be used in a keras model, we need to partition the training features from the target feature into two separate dataframes. This operation is performed within a custom function which returns a tuple of the form (X,y) where both X and y are the appropriate paritions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "498ed384830ad4030b870ca8b32c1b7e",
     "grade": false,
     "grade_id": "define_Xy_partition",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lets_do_Xy_partition( data , target_feature_name ):\n",
    "    '''\n",
    "    input: \n",
    "       * data: pandas dataframe to be partitioned\n",
    "       * target_feature_name: string name of the column\n",
    "         containing the target feature.\n",
    "    \n",
    "    returns: \n",
    "       * tuple of the form (X,y) where X is the data parameter\n",
    "         with the target feature column dropped, and y is a \n",
    "         pandas dataframe containing only the target feature.\n",
    "    '''\n",
    "    X = y = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9374ebf770fc4b51621eb550317360c",
     "grade": false,
     "grade_id": "utility_check_Xy_partition",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_Xy_partition(csv_file,target_feature):\n",
    "    '''\n",
    "    An utility function to check your definition of lets_do_Xy_partition function.\n",
    "    Returns True if the definition is correct, otherwise it returns false.\n",
    "    '''\n",
    "    data = read_dataset( csv_file )\n",
    "    data_shape_before = len(data.columns)\n",
    "    X, y = lets_do_Xy_partition(data,target_feature)\n",
    "    data_shape_after = len(data.columns)\n",
    "    \n",
    "    so_far_so_good = False\n",
    "    if data_shape_before == data_shape_after: #should not change original dataset\n",
    "        if isinstance(X, pd.DataFrame) and isinstance(y, pd.DataFrame): #both X, y needs to be dataframes\n",
    "            if X.shape[1] == data.shape[1]-1 and y.shape[1]==1:\n",
    "                 so_far_so_good = True\n",
    "    return so_far_so_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5f07ba1f448a5a95074d8609155437a",
     "grade": true,
     "grade_id": "Xy_partition_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that lets_do_Xy_partition returns the correct output for several inputs\"\"\"\n",
    "\n",
    "assert  check_Xy_partition('dataset/dataset.csv','Exited') == True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e74dd3baefd50b1e9c50cb05dc413b50",
     "grade": false,
     "grade_id": "final_Xy_partitioning",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Finally, settle down with one last Xy partitioning... where Exited column is marked as target. \"\"\"\n",
    "\n",
    "X , y = lets_do_Xy_partition( dataset_shuffled , 'Exited' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (10 points)\n",
    "### Fourth Preprocessing that we will do is Train-Test Split of X, y\n",
    "\n",
    "Now that we have X and y tables with appropriate feature pruning performed, we must split the data into a training partition (X_train, y_train) and a testing partition (X_test, y_test). \n",
    "\n",
    "The training partitions (X_train, y_train) will be used to train your model, while the test partition (X_test, y_test) will be used to evaluate the trained model. \n",
    "\n",
    "Training and test splits should be mutually exclusive to the datasets... i.e., a sample can not be both in the two splits.\n",
    "\n",
    "\n",
    "Please use `sklearn.model_selection.train_test_split` function to conduct the splits. Please do a 80-20 split, meaning 80% of the (X,y) dataset will be in (X_train, y_train) split, while, remaining 20% will be in (X_test,y_test) split. \n",
    "\n",
    "Please do not forget to initialize `random_state` parameter of the train_test_split function to the `random_seed` we defined in the first cell of this notebook. This ensures replicable results, and easier to validate your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb0058c432443baf04f59be0fe20fbd2",
     "grade": false,
     "grade_id": "train_test_split",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Use Sklearn to Split the Data into Train and Test Sets === #\n",
    "X_train = X_test = y_train = y_test = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e919aaed55fa5e591a52d8777cf3d4bc",
     "grade": false,
     "grade_id": "utility_to_check_train_test_split",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_train_test_splits(X,y,X_train,y_train,X_test,y_test):\n",
    "    '''\n",
    "    An utility function to check your work to split (X,y) into (X_train,y_train) and (X_test,y_test) splits.\n",
    "    Returns True if the definition is correct, otherwise it returns false.\n",
    "    '''\n",
    "    so_far_so_good = False\n",
    "    if isinstance(X_train, pd.DataFrame) and \\\n",
    "        isinstance(X_test, pd.DataFrame) and \\\n",
    "            isinstance(y_train, pd.DataFrame) and \\\n",
    "                isinstance(y_test, pd.DataFrame):\n",
    "        if len(X)*0.20== len(X_test) and len(y)*0.20==len(y_test):\n",
    "            so_far_so_good = True\n",
    "    return so_far_so_good\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "159a374492a11cc3566c6ffc8ad3ba81",
     "grade": true,
     "grade_id": "checking_train_test_split",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check if your work on train test split of (X,y) is successful.\"\"\"\n",
    "\n",
    "assert  check_train_test_splits(X,y,X_train,y_train,X_test,y_test) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (10 points)\n",
    "\n",
    "### Fifth preprocessing that we will do is the *Conversion of Categorical features to Numerical*\n",
    "#### Preferrably Using One Hot Encoding\n",
    "\n",
    "A little background first: Categorical features are features that contain values that are not numeric. It would be absurd to work with non-numeric features if you ask neurons in your ANN to compute the weighted sum of inputs, and then pass through activation function, right? These maths are undefined. An obvious solution you may be intrigued to do is dropping the features! Aha! Wrong!! Every piece of data is precious... may present with valuable insights of the data samples to find the patterns to map inputs with output/targets. So, we should include them. But, how?\n",
    "\n",
    "The answer is via \"Encoding\". \n",
    "There several types of encoding used in practice.\n",
    "1. **Label Encoding**, where labels are encoded as subsequent numbers. Say, for a categorical feature named \"Category\" with three categorical values: {“Cat”, “Dog” or “Zebra”} can be encoded to \"0\", \"1\", \"2\" respectively as in figure below. The issue with this type of encoding may unintentionally impose a type of ordering of the categories, that may add bias to the training.\n",
    "![label-encoding](http://54.160.44.72/STATIC_FIGS_DO_NOT_MOVE/le.png)\n",
    "2. **One Hot Encoding**, ignores the ordering of the categories all together. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1. Also, don't forget to remove the original categorical features. Here below just an example, how to convert the categorical feature called \"Category\" having the {“Cat”, “Dog” or “Zebra”} values into three new binary features: \"Cat\", \"Dog\", \"Zebra\".\n",
    "![label-encoding](http://54.160.44.72/STATIC_FIGS_DO_NOT_MOVE/ohe.png)\n",
    "\n",
    "**A note on the Dummy Variable Trap**\n",
    "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (i.e., becomes multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
    "\n",
    "Using the one-hot encoding method, a new dummy variable is created for each categorical variable to represent the presence (1) or absence (0) of the categorical variable. For example, if tree species is a categorical variable made up of the values pine, or oak, then tree species can be represented as a dummy variable by converting each variable to a one-hot vector. This means that a separate column is obtained for each category, where the first column represents if the tree is pine and the second column represents if the tree is oak. Each column will contain a 0 or 1 if the tree in question is of the column's species. These two columns are multi-collinear since if a tree is pine, then we know it's not oak and vice versa. The machine learning models trained on dataset having this multi-collinearity suffers. A remedy is to drop first (or any one) of the dummy (i.e., one-hot) features created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaca1d28180684322278bf711ccb322a",
     "grade": false,
     "grade_id": "one-hot-encoding-function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lets_do_one_hot_encoding(data, categorical_features, transform_only=True, encoders=[]):\n",
    "    '''\n",
    "    The function does one_hot_encoding on the given dataset... \n",
    "    It's intentionally defined fully, meaning you do not need to do anything here... but show us\n",
    "    how to use it properly.\n",
    "    \n",
    "    Input: \n",
    "        * data -- it's pretty much either X_train, or X_test that you prepared previously, that is\n",
    "                  any dataframe having independent variables.\n",
    "        * categorical_features -- a list of column/feature names in the data (dataframe) that you think\n",
    "                  are non-numerical / i.e., categorical that you want to be converted to numerical\n",
    "                  using the one-hot encoding technique.\n",
    "        * transform_only -- a boolean parameter, if set to False, will learn (i.e., fit) various categorical\n",
    "                values in the categorical_features from the given dataset, and use this to encode the dataset\n",
    "                using one-hot encoding. This is important that you set to False on training dataset, and\n",
    "                True on test set. If new categorical values are present in the test dataset, those will be\n",
    "                ignored, making it easier to have same set of encoded features both in training and test \n",
    "                dataset... otherwise, subsequent operations (may/) will not work.\n",
    "        * encoders -- a list of one-hot encoders previously saved, and now will be used to encode given dataset.\n",
    "                If transform_only=True, the function looks for this provided list of encoders to encode the \n",
    "                dataset instead of learning new categories. Once again, it's expected that you pass the set\n",
    "                of encoders for each of the categorical features that you encoded the training set -- i.e.,\n",
    "                leave it empty for training set, and pass the set of encoders while encoding test set.\n",
    "    Returns:\n",
    "        * data -- the converted dataframe after the encoding is completed.\n",
    "        * enc_list -- is the list of encoders used to encode the given data.\n",
    "        * categorical_features -- is the list of categorical features that you would want to encode.\n",
    "        It's expected that for training dataset, you save the encoder list and the list of features for later\n",
    "        use to encode a test dataset.\n",
    "    '''\n",
    "    if len(encoders)>0:\n",
    "        enc_list=encoders\n",
    "    else:\n",
    "        enc_list = []\n",
    "    col_onehot = []\n",
    "    i = 0\n",
    "    for feature in categorical_features:\n",
    "        if transform_only==True: #for test dataset\n",
    "            enc = enc_list[i]\n",
    "            c_onehot = enc.transform(data[[feature]])\n",
    "            i = i + 1\n",
    "        else: #fit and transform\n",
    "            enc = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "            c_onehot = enc.fit_transform(data[[feature]])\n",
    "            enc_list.append(enc)\n",
    "        c_onehot = pd.DataFrame(c_onehot, columns=list(enc.categories_[0][1:])) #dropped first column\n",
    "        col_onehot.append(c_onehot)\n",
    "        \n",
    "    #concat all onehot feature columns\n",
    "    concat_df = pd.concat(col_onehot,axis=1)\n",
    "    #match index with given data\n",
    "    concat_df.index = data.index\n",
    "    #drop the categorical feature columns\n",
    "    data = lets_drop_features( data , features_to_drop=categorical_features )\n",
    "    #merge with those new onehot features\n",
    "    data = data.join(concat_df)\n",
    "    return data,enc_list,categorical_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28b1820070970ec8b5740cdab0e4ff3d",
     "grade": false,
     "grade_id": "onehot_encoding_training_set",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's apply one-hot encoding on two columns: \"Geography\" and \"Gender\" of the training dataset, X_train.\n",
    "Please make good use the function, lets_do_one_hot_encoding() defined above.\n",
    "Also, receive the 3 return values from the function call to use them in the next question, that is,\n",
    "encoding the test dataset, X_test. It's always a good idea to save the list of encoders and features\n",
    "in file, if you would want to apply your model to evaluate/predict on a new test sample.\n",
    "\n",
    "'''\n",
    "X_train_ohe = []  #one-hot encoded training dataset\n",
    "enc_list = [] #encoder list to be created during one-hot encoding of the training dataset\n",
    "categorical_features = [] #The list of categorical features in question. The function call\n",
    "                          #should return [\"Geography\", \"Gender\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4960fee0ebfed27553de2e8f12ecfc72",
     "grade": false,
     "grade_id": "onehot_encoding_test_set",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's apply one-hot encoding on the same two columns: \"Geography\" and \"Gender\" of the test dataset, X_test.\n",
    "Please make good use the function, lets_do_one_hot_encoding() defined above.\n",
    "Also, receive the 3 return values from the function call. Although, the second and third returned values\n",
    "can be ignored, if you like.\n",
    "\n",
    "Please make sure, the one-hot encoded training set and test set has the exact same number of features/columns,\n",
    "and in the same order. If not, the test in the next cell will almost certainly fail, and you will lose points,\n",
    "and the training and testing of the Artificial Neural Network will absolutely fail. So, please take close\n",
    "attention.\n",
    "\n",
    "'''\n",
    "X_test_ohe = []  #one-hot encoded training dataset\n",
    "\n",
    "    \n",
    "    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11098c7fdd373702f692e0215c9f0744",
     "grade": false,
     "grade_id": "utility_for_ohot_encoding",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_one_hot_encoding(X_train_ohe,X_test_ohe):\n",
    "    '''\n",
    "    This utility function checks if one-hot encoding of training and test sets are correctly done.\n",
    "    Returns true if its correct, otherwise returns false.\n",
    "    '''\n",
    "    \n",
    "    so_far_so_good = True\n",
    "    if isinstance(X_train_ohe,pd.DataFrame) and isinstance(X_test_ohe,pd.DataFrame):\n",
    "        if (X_train_ohe.columns == X_test_ohe.columns).all == False:\n",
    "            so_far_so_good = False\n",
    "    else:\n",
    "        so_far_so_good = False\n",
    "\n",
    "    return so_far_so_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28e0af387dd589eeac9484f377533fb6",
     "grade": true,
     "grade_id": "check_one_hot_encoding",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check if your work on One-hot encoding is successful.\"\"\"\n",
    "\n",
    "assert  check_one_hot_encoding(X_train_ohe,X_test_ohe) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: (10 points)\n",
    "\n",
    "### Sixth Preprocessing that we are going to do is *Normalization of X_train_ohe, and X_test_ohe*\n",
    "\n",
    "Now that we all numerical training and test dataset, X_train_ohe and X_test_ohe respectively, we can normalize each features in both the dataset. **Normalization** is just one of the way to scale each feature. In class you'll learn a ton of other ways to scale. For this task, let's resort to **Normalization**.\n",
    "\n",
    "\"The rule of thumb for scaling datasets, is we scale training dataset first, then using the statistics that we learn during the scaling process, we scale the test dataset. We do not learn any statistics from the test dataset.\"\n",
    "\n",
    "Also, scaling is commonly performed column-wise, and never sample/row wise.\n",
    "\n",
    "\n",
    "Let's use normalization scheme from the package, `sklearn.preprocessing.Normalizer`. You could do this manually by subtracting mean of a column, followed by dividing by standard deviation of that column. You repeat this for all the columns in the given dtaset. \n",
    "Anyway, be sure to save the normalizer object that you will fit and transform the training set so that you first use it to scale the test set, or anytime a new test set is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ecfc1f9bc69f21c959319b7b65bcc3a",
     "grade": false,
     "grade_id": "normalizer-learning-training",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Fit (i.e., learn) Sklearn's Normalizer to Our Data === #\n",
    "normalizer = StandardScaler( ) #Construct the normalizer object\n",
    "\n",
    "normalizer.fit( X_train_ohe ) # Fit (i.e., learn from the training dataset, X_train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd29d12bf25e3b534daa9029faea8325",
     "grade": false,
     "grade_id": "scaling-train-test",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Using the fitted normalizer object's member method, transform() normalize the training dataset, X_train_ohe.\n",
    "Then, normalize the test dataset, X_test_ohe.\n",
    "Save the scaled training and test sets into X_train_scaled, and X_test_scaled variables.\n",
    "'''\n",
    "\n",
    "X_train_scaled = []  \n",
    "X_test_scaled = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "350e58a07cbc688dd1e85e773bd336df",
     "grade": false,
     "grade_id": "check_for_scaling",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_the_scaling(dataset):\n",
    "    '''\n",
    "    After normalization, mean of every column would be 0.0 (or close enough), and standard deviation 1.0\n",
    "    This function will fail if you do something different.\n",
    "    '''\n",
    "    so_far_so_good = False\n",
    "    if ((dataset.mean() - 0.0) < 0.0000001) and ((dataset.std() - 1.0) < 0.0000001):\n",
    "        so_far_so_good = True\n",
    "    \n",
    "    return so_far_so_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eba5434807c518b6f0f0a7fd64a1c531",
     "grade": true,
     "grade_id": "check_normalization_success",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check if your work on scaling of both X_train_ohe and X_test_ohe was in fact successful.\"\"\"\n",
    "\n",
    "assert  check_the_scaling(X_train_scaled) == True\n",
    "\n",
    "\"\"\"Also, you will have left 11 columns for each of the datasets.\"\"\"\n",
    "assert X_train_scaled.shape[1] == 11 and X_test_scaled.shape[1] == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing an ANN Using Keras\n",
    "\n",
    "Design an artificial neural network, ANN using the Keras library. For reference, please take a look here [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/). For simplicity, please use the `adam` optimizer and `binary_crossentropy` as the loss function the optimizer will use. Details will be presented later. Here, just use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 1.1: Construct the Layers\n",
    "\n",
    "The first step to define a model in Keras is to define the layers of neurons used by the model. The following cell will define each of these layers for the first network model, having the following architecture:\n",
    "\n",
    "* Input layer will have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* First hidden layer will have 5 neurons, each with \"ReLU\" as activation function (the popular one).\n",
    "* Second hidden layer will have 4 neurons, each with \"ReLU\" as activation function.\n",
    "* Output layer will have just 1 neuron, with sigmoid activation function. The reason being, output of this neuron will tell the probability score of the target outcome: \"Exited\" True or False. If the output neuron produces value above 0.5, we will say the neural network predicted \"True\", otherwise, False. THis is the beauty of using sigmoid function at the output layer as we can interpret the output as if it's probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45b57b71b0390a13a825f2662ba1bc19",
     "grade": false,
     "grade_id": "ANN-1_layers",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "    keras.Input( shape=( X_train_scaled[0].shape ) ),  # Input Layer\n",
    "    keras.layers.Dense(  5 , activation='relu'    , name='hidden-layer-1' ),  \n",
    "    keras.layers.Dense(  4 , activation='relu'    , name='hidden-layer-2' ),  \n",
    "    keras.layers.Dense(  1 , activation='sigmoid' , name='output-layer' )  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 1.2: Compile the Model\n",
    "\n",
    "Now that we have the layers list prepared, we must call compile() with specifications like which loss function to optimize to learn the weights of the network, and the name of the optimizer function. There are variations of gradient descent algorithm. You can also specify which evaluation metric to show during the training process. \n",
    "\n",
    "Let's optimize the `binary_corssentropy` as the loss function using the `adam` optimizer. We want the training function, i.e., `fit()` prints model accuracy for every epoch.\n",
    "\n",
    "Also, let's print the compiled model which is ready to learn from the training dataset you would present to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "760ab7bd79ff184f18d5579bfbee3ab0",
     "grade": false,
     "grade_id": "ANN-1-compile",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Define the Model Object === #\n",
    "ann_1 = keras.Sequential( layers=layers )\n",
    "\n",
    "# === Compile the Model With Appropriate Parameters === #\n",
    "ann_1.compile( loss='binary_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'] )  # metric shown during training\n",
    "\n",
    "# === Show Model Compiled Successfully by Printing Summary === #\n",
    "ann_1.summary( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 1.3 Train the Model\n",
    "\n",
    "Let's start the training the training dataset, X_train_scaled by calling the `fit()` function through the ANN model, ANN-1. \n",
    "\n",
    "For simplicity, let's train for 25 epochs. If you are ambitious, you should run it longer! But, be warned, if you are running it without GPU support enabled at the beginning of the notebook, you need to wait a lot longer.\n",
    "\n",
    "If you are curious to know the details what happened during the training, saving history, i.e, return value from the `fit()` may be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7796957a0677496c980bc255f22288c1",
     "grade": false,
     "grade_id": "ANN-1-fit",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Train While Recording History for Visualization === #\n",
    "ann_1_history = ann_1.fit( X_train_scaled, \n",
    "                           y_train, \n",
    "                           epochs=25,\n",
    "                           validation_data=( X_test_scaled , y_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 1.3 : Training Visualization\n",
    "\n",
    "Let's visualize the model's training in order to achieve a better understanding of whether overfitting/underfitting may have occurred, or check on the performance in general (i.e., conduct a simple sanity check of the training). \n",
    "\n",
    "As observed in the accuracy chart, we begin to see a slight decline in the test accuracy after about 20 epochs, showing that there is overfitting occurring. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37465f4fbb7a0ba738301fc3846114bd",
     "grade": false,
     "grade_id": "ANN-1-visualization",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Plot the Accuracy === #\n",
    "plt.figure( )\n",
    "plt.plot( ann_1_history.history['accuracy']     , label='Training Accuracy' )\n",
    "plt.plot( ann_1_history.history['val_accuracy'] , label='Testing Accuracy' )\n",
    "\n",
    "# === Label the Chart === #\n",
    "plt.xlabel( 'Epoch' )\n",
    "plt.ylabel( 'Accuracy' )\n",
    "plt.title( 'Accuracy During Training' )\n",
    "plt.legend( )\n",
    "\n",
    "# === Display the Chart === #\n",
    "plt.show( )\n",
    "\n",
    "\n",
    "# === Plot the Loss === #\n",
    "plt.figure( )\n",
    "plt.plot( ann_1_history.history['loss']     , label='Training Loss' )\n",
    "plt.plot( ann_1_history.history['val_loss'] , label='Testing Loss' )\n",
    "\n",
    "# === Label the Chart === #\n",
    "plt.xlabel( 'Epoch' )\n",
    "plt.ylabel( 'Loss' )\n",
    "plt.title( 'Loss During Training' )\n",
    "plt.legend( )\n",
    "\n",
    "# === Display the Chart === #\n",
    "plt.show( )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 1.4: Evaluate the Model\n",
    "\n",
    "Finally, evaluate the performance of ANN-1 based model in terms of accuracy, precision, recall, and F1 scores.\n",
    "\n",
    "To do this, we will implement two helper functions. First is `get_confusion_matrix`, which does exactly what is expected. It compares the predictions to the ground truth in order to determine the count of true positives, false positives, true negatives, and false negatives. The function returns the matrix in dictionary format to be used by the other function `evaluate_model`, which given a model, the test data, and the test labels, will predict the labels, compute the confusion matrix from the predictions, then compute accuracy, precision, recall, and F1. The function then returns all computed metrics in the form of a dictionary. This function returns the results in order to programatically determine the best trained model to be used against the judgement set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca14d64a4312353b97c858c6ffbe8019",
     "grade": false,
     "grade_id": "utility-evaluation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_confusion_matrix( y , y_pred ):\n",
    "    '''\n",
    "    Input: \n",
    "       * y: ground truth labels\n",
    "       * y_pred: predicted labels\n",
    "    \n",
    "    Returns: \n",
    "       * dictionary containing the true positives (TP),\n",
    "         false positives (FP), true negatives (TN), and\n",
    "         false negatives (FN) containing counts of each\n",
    "         metric based on the inputs\n",
    "    \n",
    "    Task:\n",
    "       * iterate the two lists (assumed to be the same length)\n",
    "       * count TP, FP, FN, TN based on the lists\n",
    "       * return appropriately formatted dictionary\n",
    "    '''\n",
    "    \n",
    "    # === Initialize Empty Matrix === #\n",
    "    conf_mat = { 'TP':0, 'FP':0, 'TN':0, 'FN':0 }\n",
    "    \n",
    "    # === Iterate Lists to Count Metrics === #\n",
    "    for i in range( len( y ) ):\n",
    "        \n",
    "        # === True Positive === #\n",
    "        if y[i] == 1 and y_pred[i] == 1:\n",
    "            conf_mat['TP'] += 1\n",
    "            \n",
    "        # === False Negative === #\n",
    "        elif y[i] == 1 and y_pred[i] == 0:\n",
    "            conf_mat['FN'] += 1\n",
    "            \n",
    "        # === False Positive === #\n",
    "        elif y[i] == 0 and y_pred[i] == 1:\n",
    "            conf_mat['FP'] += 1\n",
    "            \n",
    "        # === True Negative === #\n",
    "        elif y[i] == 0 and y_pred[i] == 0:\n",
    "            conf_mat['TN'] += 1\n",
    "            \n",
    "    # === Return the Results === #\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def evaluate_model( model , X_train , y_train , X_test , y_test , tolerance=0.5 ):\n",
    "    '''\n",
    "    input: \n",
    "       * model: trained keras model to evaluate\n",
    "       * X_train: training dataset\n",
    "       * y_train: ground truth labels\n",
    "       * X_test: test dataset\n",
    "       * y_test: ground truth labels\n",
    "       * tolerance (optional): level of confidence for prediction\n",
    "            - default = 0.5, which is common with sigmoid enabled output neurons.\n",
    "    \n",
    "    returns: \n",
    "       * dictionary containing the confusion matrix (conf_mat),\n",
    "         accuracy (accuracy), precision (precision), recall (recall),\n",
    "         and F1 score (F1) with respect to the test dataset given\n",
    "         (for both training and test sets)\n",
    "    \n",
    "    Task:\n",
    "       * use model to predict labels\n",
    "       * use prediction and given truth to compute confusion matrix\n",
    "       * use confusion matrix to compute the metrics\n",
    "       * repeat for training metrics\n",
    "       * return the results with appropriate formatting\n",
    "    '''\n",
    "    \n",
    "    # === Predict the Target === #\n",
    "    y_pred       = model.predict( X_test )\n",
    "    y_pred_train = model.predict( X_train )\n",
    "    \n",
    "    # === Convert Numerical \"Confidence\" to Classification Value === #\n",
    "    y_pred[y_pred >= tolerance] = 1\n",
    "    y_pred[y_pred <  tolerance] = 0\n",
    "    y_pred_train[y_pred_train >= tolerance] = 1\n",
    "    y_pred_train[y_pred_train <  tolerance] = 0\n",
    "    \n",
    "    # === Convert to Integer Arrays Instead of Floats === #\n",
    "    y_pred  = np.array( y_pred , dtype=int )\n",
    "    y_test  = np.array( y_test , dtype=int )\n",
    "    y_train = np.array( y_train , dtype=int )\n",
    "    \n",
    "    # === Get the Confusion Matrix === #\n",
    "    conf_mat       = get_confusion_matrix( y_test , y_pred )\n",
    "    conf_mat_train = get_confusion_matrix( y_train , y_pred_train )\n",
    "    \n",
    "    # === Metrics Formulae === #\n",
    "    accuracy  = lambda tp,tn,fp,fn: ( tp + tn ) / ( tp + tn + fp + fn )\n",
    "    precision = lambda tp,fp: tp / ( tp + fp )\n",
    "    recall    = lambda tp,fn: tp / ( tp + fn )\n",
    "    f1_score  = lambda p,r: ( 2 * p * r ) / ( p + r )\n",
    "    eps       = 1e-8  # prevent divide by zero\n",
    "    \n",
    "    # === Compute Metrics === #\n",
    "    metrics = {\n",
    "        # Test Metrics\n",
    "        'conf_mat'  : conf_mat,\n",
    "        'accuracy'  : accuracy( conf_mat['TP']+eps , conf_mat['TN']+eps , conf_mat['FP']+eps , conf_mat['FN']+eps ),\n",
    "        'precision' : precision( conf_mat['TP']+eps , conf_mat['FP']+eps ),\n",
    "        'recall'    : recall( conf_mat['TP']+eps , conf_mat['FN']+eps ),\n",
    "        \n",
    "        # Training Metrics\n",
    "        'conf_mat_train' : conf_mat_train,\n",
    "        'accuracy_train'  : accuracy( conf_mat_train['TP']+eps , conf_mat_train['TN']+eps , conf_mat_train['FP']+eps , conf_mat_train['FN']+eps ),\n",
    "        'precision_train' : precision( conf_mat_train['TP']+eps , conf_mat_train['FP']+eps ),\n",
    "        'recall_train'    : recall( conf_mat_train['TP']+eps , conf_mat_train['FN']+eps )\n",
    "    }\n",
    "    metrics['F1']       = f1_score( metrics['precision']+eps , metrics['recall']+eps )\n",
    "    metrics['F1_train'] = f1_score( metrics['precision_train']+eps , metrics['recall_train']+eps )\n",
    "    \n",
    "    # === Return Metrics === #\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0cad8b32f3121a6a5ceba29c0efc3fb",
     "grade": false,
     "grade_id": "ANN-1-evaluation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Extract Metrics, Archive, and Report === #\n",
    "ann_1_metrics = evaluate_model( ann_1 , X_train_scaled , y_train , X_test_scaled , y_test )\n",
    "model_metrics = [ ann_1_metrics ]\n",
    "model_objects = [ ann_1 ]\n",
    "ann_1_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: (10 points)\n",
    "\n",
    "### Keras ANN 2.1: Construct the Layers\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 8 neurons, with relu activation,\n",
    "* Hidden-layer-3: 8 neurons, with relu activation,\n",
    "* Hidden-layer-4: 8 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "723bc57a61e38a2e77b3e66aa97e1435",
     "grade": false,
     "grade_id": "ANN-2-layers",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make sure to put all the Keras layers in the layers_2 variable below.\n",
    "'''\n",
    "layers_2 = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 2.2: Compile the Model\n",
    "\n",
    "Here, we first created the ANN with the layers according to the specification above, and the network\n",
    "will be saved in `ann_2` object.\n",
    "\n",
    "Similarly to the first model, you need to compile the model with the `adam` optimizer and the `binary_corssentropy` loss function. And, pass 'accuracy' to be shown during every epoch run of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c50ade451fe551c2e3d4b5f35a040359",
     "grade": false,
     "grade_id": "ANN-2-compile",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Define the Model Object === #\n",
    "ann_2 = keras.Sequential( layers=layers_2 )\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c5eede092b087be70a919b4f47ed831",
     "grade": false,
     "grade_id": "utility-check_network_architecture",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_network_architecture(model, expected_layer_shapes, \n",
    "                               expected_params,\n",
    "                               expected_loss_function\n",
    "                              ):\n",
    "    layer_shapes = []\n",
    "    for layer in model.layers:\n",
    "        layer_shapes.append(layer.output_shape[1])\n",
    "\n",
    "    so_far_so_good = False\n",
    "    if layer_shapes == expected_layer_shapes:\n",
    "        if model.count_params()==expected_params:\n",
    "            if model.loss == expected_loss_function:\n",
    "                so_far_so_good = True\n",
    "    return so_far_so_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e26635e01d7ab74cc527843c61c8af9d",
     "grade": true,
     "grade_id": "ANN-2-compile-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check if ANN-2 network is compiled according to the specification\"\"\"\n",
    "\n",
    "assert  check_network_architecture(ann_2, \n",
    "                                  expected_layer_shapes=[8,8,8,8,1],\n",
    "                                  expected_params=321,\n",
    "                                  expected_loss_function='binary_crossentropy') == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 2.3: Train the Model\n",
    "\n",
    "* Train ANN-2 with the training set, X_train_scaled, and use X_test_scaled for validation.\n",
    "* Continue training for 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90b6558120f854a124732d9196b3818b",
     "grade": false,
     "grade_id": "ANN-2-training",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Train While Recording History for Visualization === #\n",
    "ann_2_history = ann_2.fit( X_train_scaled, \n",
    "                           y_train, \n",
    "                           epochs=25,\n",
    "                           validation_data=( X_test_scaled , y_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 2.3: Training Visualization\n",
    "\n",
    "Let's visualize the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "871deaefff63201fa7bf361cb72b7b73",
     "grade": false,
     "grade_id": "ANN-2-fit-visualization",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Plot the Accuracy === #\n",
    "plt.figure( )\n",
    "plt.plot( ann_2_history.history['accuracy']     , label='Training Accuracy' )\n",
    "plt.plot( ann_2_history.history['val_accuracy'] , label='Testing Accuracy' )\n",
    "\n",
    "# === Label the Chart === #\n",
    "plt.xlabel( 'Epoch' )\n",
    "plt.ylabel( 'Accuracy' )\n",
    "plt.title( 'Accuracy During Training' )\n",
    "plt.legend( )\n",
    "\n",
    "# === Display the Chart === #\n",
    "plt.show( )\n",
    "\n",
    "\n",
    "# === Plot the Loss === #\n",
    "plt.figure( )\n",
    "plt.plot( ann_2_history.history['loss']     , label='Training Loss' )\n",
    "plt.plot( ann_2_history.history['val_loss'] , label='Testing Loss' )\n",
    "\n",
    "# === Label the Chart === #\n",
    "plt.xlabel( 'Epoch' )\n",
    "plt.ylabel( 'Loss' )\n",
    "plt.title( 'Loss During Training' )\n",
    "plt.legend( )\n",
    "\n",
    "# === Display the Chart === #\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 2.4: Evaluate the Model\n",
    "\n",
    "Compute the same evaluation metrics for ANN-2 based model on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract Metrics, Archive, and Report === #\n",
    "ann_2_metrics = evaluate_model( ann_2 , X_train_scaled , y_train , X_test_scaled , y_test )\n",
    "model_metrics.append( ann_2_metrics )\n",
    "model_objects.append( ann_2 )\n",
    "ann_2_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: (10 points)\n",
    "\n",
    "### Keras ANN 3.1: Construct the Layers\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 4 neurons, with relu activation,\n",
    "* Hidden-layer-3: 2 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5ba639d982ddc1e995c5d4bb29f016c",
     "grade": false,
     "grade_id": "ANN-3-layers",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make sure to put all the Keras layers in the layers_3 variable below.\n",
    "'''\n",
    "layers_3 = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 3.2: Compile the Model\n",
    "\n",
    "Here, we first created the ANN with the layers according to the specification above, and the network\n",
    "will be saved in `ann_3` object.\n",
    "\n",
    "Similarly to the first and second models, you need to compile the model with the `adam` optimizer and the `binary_corssentropy` loss function. And, pass 'accuracy' to be shown during every epoch run of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75c9ab33cdfce0b2320ac361c7def02a",
     "grade": false,
     "grade_id": "ANN-3-compile",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Define the Model Object === #\n",
    "ann_3 = keras.Sequential( layers=layers_3 )\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aab56c152b86bef97dd1c39cfbdaa2fb",
     "grade": true,
     "grade_id": "ANN-3-compile-check",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check if ANN-3 network is compiled according to the specification\"\"\"\n",
    "\n",
    "assert  check_network_architecture(ann_3, \n",
    "                                  expected_layer_shapes=[8,4,2,1],\n",
    "                                  expected_params=145,\n",
    "                                  expected_loss_function='binary_crossentropy') == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 3.3: Train the Model\n",
    "\n",
    "Now that the model is compiled, it must be trained. In order to compare the model structures directly, both will be trained for the same number of epochs (i.e., 25) and evaluated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train While Recording History for Visualization === #\n",
    "ann_3_history = ann_3.fit( X_train_scaled, \n",
    "                           y_train, \n",
    "                           epochs=25,\n",
    "                           validation_data=( X_test_scaled , y_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 3.3: Training Visualization\n",
    "\n",
    "Let's visualize the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot the Accuracy === #\n",
    "plt.figure( )\n",
    "plt.plot( ann_3_history.history['accuracy']     , label='Training Accuracy' )\n",
    "plt.plot( ann_3_history.history['val_accuracy'] , label='Testing Accuracy' )\n",
    "\n",
    "# === Label the Chart === #\n",
    "plt.xlabel( 'Epoch' )\n",
    "plt.ylabel( 'Accuracy' )\n",
    "plt.title( 'Accuracy During Training' )\n",
    "plt.legend( )\n",
    "\n",
    "# === Display the Chart === #\n",
    "plt.show( )\n",
    "\n",
    "\n",
    "# === Plot the Loss === #\n",
    "plt.figure( )\n",
    "plt.plot( ann_3_history.history['loss']     , label='Training Loss' )\n",
    "plt.plot( ann_3_history.history['val_loss'] , label='Testing Loss' )\n",
    "\n",
    "# === Label the Chart === #\n",
    "plt.xlabel( 'Epoch' )\n",
    "plt.ylabel( 'Loss' )\n",
    "plt.title( 'Loss During Training' )\n",
    "plt.legend( )\n",
    "\n",
    "# === Display the Chart === #\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras ANN 3.4: Evaluate the Model\n",
    "\n",
    "In order to compare the keras models, and to determine which one to apply on the judgement set, we must compute the same metrics for ANN-3. We can utilized the functions defined earlier to obtain evaluation metrics in the same format as those computed for ANN-1 and ANN-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract Metrics, Archive, and Report === #\n",
    "ann_3_metrics = evaluate_model( ann_3 , X_train_scaled , y_train , X_test_scaled , y_test )\n",
    "model_metrics.append( ann_3_metrics )\n",
    "model_objects.append( ann_3 )\n",
    "ann_3_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: (10 points)\n",
    "### Recreating ANNs Without Keras library -- defining your own ANN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ANN Class\n",
    "\n",
    "In order to train a custom neural network, we must create a wrapper class in order to streamline the operations of the network and ensure the recreation of the keras models is done as \"painlessly\" as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c41dd904b19726b540150ebe7b22c8b3",
     "grade": false,
     "grade_id": "MyANN-class-definition",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# CLASS: MyANN\n",
    "# ==================================================\n",
    "# ATTRIBUTES:\n",
    "# ==================================================\n",
    "#\n",
    "#   - input_shape: tuple containing shape of input data\n",
    "#   - weight: list of numpy arrays (matrices) containing\n",
    "#             weights of the perceptrons for each layer\n",
    "#   - bias: list of biases for each layer of the network\n",
    "#   - activation: strings containing the activation function for\n",
    "#                 the corresponding layer. Only relu and\n",
    "#                 sigmoid supported.\n",
    "#   - functions: dictionary that maps the name of the\n",
    "#                activation functions to their corresponding\n",
    "#                member function.\n",
    "#   - outputs: the (post activation) outputs of each layer\n",
    "#              corresponding to the last call of \n",
    "#              self.feed_forward(). intended for internal\n",
    "#              use only\n",
    "#   - deltas: the error to be propagated into the corresponding\n",
    "#             layers of the network at the most recent training\n",
    "#             step. Intended for internal use only\n",
    "#\n",
    "# ==================================================\n",
    "# CONSTRUCTOR\n",
    "# ==================================================\n",
    "#\n",
    "# input: \n",
    "#   * N/A\n",
    "#\n",
    "# output: \n",
    "#   * N/A\n",
    "#\n",
    "# Task:\n",
    "#   * initialize attributes and seed the random\n",
    "#     numbers for consistent replication.\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: relu( x )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - x: the numpy array to apply the relu function to\n",
    "#\n",
    "# Output:\n",
    "#   - the results of performing relu on the input\n",
    "#\n",
    "# Task:\n",
    "#   - define the relu function:\n",
    "#        f(x) = max(0,x)\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: sigmoid( x )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - x: numpy array to apply the sigmoid function to\n",
    "#\n",
    "# Output:\n",
    "#   - return the results of passing the input through\n",
    "#     the sigmoid function\n",
    "#\n",
    "# Task:\n",
    "#   - Pass the given input into the sigmoid function\n",
    "#        f(x) = 1 / ( 1 - e^(-x) )\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: add_input( input_shape )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - input_shape: tuple containing the shape of the input\n",
    "#\n",
    "# Output:\n",
    "#   - N/A\n",
    "#\n",
    "# Task:\n",
    "#   - set the input_shape attribute to match input\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: add_layer( n_nodes , activation )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - n_nodes: integer representing number of nodes in\n",
    "#              the layer being added\n",
    "#   - activation: string indicating which activation\n",
    "#              function to use\n",
    "#\n",
    "# Output:\n",
    "#   - N/A\n",
    "#\n",
    "# Task:\n",
    "#   - Add an appropriately shapped matrix to the\n",
    "#     weight attribute for the connections to the\n",
    "#     previous layer. Weights randomly initialized\n",
    "#   - Add a bias of 1 to the previous layer\n",
    "#   - Add the current layer's activation\n",
    "#   - \n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: compute_loss_gradient( pred , truth )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - pred: the computed value of the network output\n",
    "#   - truth: the expected output of the network\n",
    "#\n",
    "# Output:\n",
    "#   - the results of the gradient for binary corssentropy\n",
    "#     (gradient used for minimizing that loss function)\n",
    "#\n",
    "# Task:\n",
    "#   - return the gradient of the binary crossentropy \n",
    "#        * log-loss\n",
    "#\n",
    "# Notes:\n",
    "#   - Binary Crossentropy Function:\n",
    "#       f(x) = -y*log(z) - (1-y)*log(1-z)\n",
    "#       Where y=truth, z=pred\n",
    "#   - Piecewise Binary Crossentropy:\n",
    "#       f(x) = { -log(z) if y=1; -log(1-z) otherwise; }\n",
    "#   - Gradient of Binary Crossentropy:\n",
    "#       f(x) = -(y/z) + (1-y)/(1-z)\n",
    "#   - Piecewise Gradient of Binary Crossentropy:\n",
    "#       f(x) = { -1/z if y=1; 1/(1-z) otherwise; }\n",
    "#   - The piecewise gradient is used for this function\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: transfer_derivative( output , activation )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - output: the output of the neurons in the layer being\n",
    "#             transferred\n",
    "#   - activation: the activation function the neurons used\n",
    "#\n",
    "# Output:\n",
    "#   - return the determined scale by which the error should be\n",
    "#     transferred to the previous weights in the network\n",
    "#\n",
    "# Task:\n",
    "#   - Using chain rule on the activation function, determine \n",
    "#     the factor by which the loss should be transferred\n",
    "#     to the previous layer in backpropagation\n",
    "#   - Return this value\n",
    "#     \n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: feed_forward( row )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - row: row of the data to be passed through\n",
    "#          the feed forward algorithm\n",
    "#\n",
    "# Output:\n",
    "#   - the result of a series of biased matrix multiplications\n",
    "#     starting with the given row\n",
    "#\n",
    "# Task:\n",
    "#   - multiply the row through the weights matricies with\n",
    "#     dot product, activate the neurons, apply the bias,\n",
    "#     and treat those values as inputs to the next layer\n",
    "#   - archive each layer's output and an initialized delta\n",
    "#     as member attributes for use in training.\n",
    "#   - return the final results of the final activation\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: back_propagate( row , target )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - row: data row being used for backpropagation\n",
    "#   - target: expected output used for computing loss\n",
    "#\n",
    "# Output:\n",
    "#   - N/A\n",
    "#\n",
    "# Task:\n",
    "#   - compute the overall loss gradient\n",
    "#   - using chain rule, pass the loss backwards\n",
    "#     through layers with appropriate scaling\n",
    "#   - record these values for each layer as the\n",
    "#     deltas member attribute for weight updating\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: update_weights( row , learning_rate )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - row: data row being used in current training step\n",
    "#   - learning rate: scale by which computed deltas \n",
    "#        affect weights.\n",
    "#\n",
    "# Output:\n",
    "#   - N/A\n",
    "#\n",
    "# Task:\n",
    "#   - Iterate through the layers of the network\n",
    "#   - scale each weight by learning rate times \n",
    "#     backpropagation delta times degree of the \n",
    "#     input affecting that weight\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: fit( X_train , y_train , epochs , learning_rate )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - X_train: dataset to be used for training\n",
    "#   - y_train: target labels corresponding to X_train\n",
    "#   - epochs: number of epochs (complete passes through dataset)\n",
    "#   - learning rate (optional): speed of learning\n",
    "#        * default = None\n",
    "#        * if None, it's initialized to 1/(2*num_weights)\n",
    "#          meaning, it is proportional to the number of weights\n",
    "#          to be trained if not statically specified\n",
    "#\n",
    "# Output:\n",
    "#   - N/A\n",
    "#\n",
    "# Task:\n",
    "#   - Iterate through the rows of X_train a total of \n",
    "#     epochs times\n",
    "#   - for each row, backpropagate that row, and update\n",
    "#     the weights based on backpropagation results\n",
    "#        * Stochastic Gradient Descent -- update each row\n",
    "#\n",
    "# ==================================================\n",
    "# MEMBER FUNCTION: predict( X )\n",
    "# ==================================================\n",
    "#\n",
    "# Input:\n",
    "#   - X: dataset to be predicted (assumed to be\n",
    "#        appropriately shaped rows)\n",
    "#\n",
    "# Output:\n",
    "#   - Numpy 2d array where each row contains the predicted\n",
    "#     target variable for each corresponding row in X\n",
    "#\n",
    "# Task:\n",
    "#   - perform the feed_forward function for each row\n",
    "#     row of X without the archiving for training\n",
    "#   - format the results appropriately and return\n",
    "#\n",
    "# ==================================================\n",
    "class MyANN:\n",
    "    \n",
    "    # ==============================================\n",
    "    # CONSTRUCTOR\n",
    "    # ==============================================\n",
    "    def __init__( self ):\n",
    "        np.random.seed( random_seed )\n",
    "        self.input_shape = None\n",
    "        self.weight      = [ ]\n",
    "        self.bias        = [ ]\n",
    "        self.activation  = [ ]\n",
    "        self.functions   = {\n",
    "            'relu'    : self.relu,\n",
    "            'sigmoid' : self.sigmoid\n",
    "        }\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.relu( x )\n",
    "    # ==============================================\n",
    "    def relu( self , x ):\n",
    "        tmp = x.copy( )\n",
    "        tmp[tmp <= 0] = 0  # can't use max() because \"truth value of an array is unknown\"\n",
    "        return tmp\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.sigmoid( x )\n",
    "    # ==============================================\n",
    "    def sigmoid( self , x ):\n",
    "        return 1 / ( 1 + np.exp( x ) )\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.add_input( input_shape )\n",
    "    # ==============================================\n",
    "    def add_input( self , input_shape ):\n",
    "        self.input_shape = input_shape\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.add_layer( n_nodes , activation )\n",
    "    # ==============================================\n",
    "    def add_layer( self , n_nodes , activation ):\n",
    "        \n",
    "        # === If First Hidden Layer, Use Input Shape === #\n",
    "        if len( self.weight ) == 0:\n",
    "            if self.input_shape is None:  # dismiss if no input layer\n",
    "                return -1\n",
    "            prev_shape = self.input_shape\n",
    "        \n",
    "        # === Otherwise, Use Shape of Previous Layer's Output === #\n",
    "        else:\n",
    "            prev_shape = self.weight[-1].shape[-1]\n",
    "            \n",
    "        # === Assign Bias=1 and Random Weights of Specified Dimension with Activation Given === #\n",
    "        bias    = 1\n",
    "        weights = np.random.rand( prev_shape+1 , n_nodes ) / (prev_shape+1) # +1 for bias\n",
    "        \n",
    "        # === Archive the Values === #\n",
    "        self.weight.append( weights )\n",
    "        self.bias.append( 1 )  # simple network, all biases are 1\n",
    "        self.activation.append( activation )  # assuming string\n",
    "        return  # MyANN.add_layer( )\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.compute_loss_gradient( pred , truth )\n",
    "    # ==============================================\n",
    "    def compute_loss_gradient( self , pred , truth ):  # gradient of binary crossentropy\n",
    "        \n",
    "        # === y=1: -1/z (or -inf if z is zero to prevent error) === #\n",
    "        if truth == 1:\n",
    "            if pred == 0:  # avoid divide by 0\n",
    "                return -np.inf\n",
    "            else:\n",
    "                return -1 / pred\n",
    "            \n",
    "        # === y=0: 1/(1-z) (or +inf if z is 1 to prevent error) === #\n",
    "        else:\n",
    "            if pred == 1:  # avoid divide by 0\n",
    "                return np.inf\n",
    "            else:\n",
    "                return 1 / ( 1 - pred )\n",
    "            \n",
    "            \n",
    "    # ==============================================\n",
    "    # MyANN.transfer_derivative( output , activation )\n",
    "    # ==============================================\n",
    "    def transfer_derivative( self , output , activation ):\n",
    "        \n",
    "        # === ReLU Chain Rule: 0 if Output was 0, 1 Otherwise === #\n",
    "        if activation == 'relu':\n",
    "            tmp = output.copy( )\n",
    "            tmp[tmp <= 0] = 0\n",
    "            tmp[tmp >  0] = 1\n",
    "            return tmp  # derivative of relu: linear when >0 flat <=0\n",
    "        \n",
    "        # === Sigmoid Chain Rule: s(x)*(1-s(x)) === #\n",
    "        elif activation == 'sigmoid':\n",
    "            return output * ( 1 - output )\n",
    "        \n",
    "        # === Failsafe for Invalid Activations === #\n",
    "        else:\n",
    "            return 0  # shouldn't ever get here, but just in case\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.feed_forward( row )\n",
    "    # ==============================================\n",
    "    def feed_forward( self , row ):\n",
    "        \n",
    "        # === Initialization === #\n",
    "        self.bias[-1] = 0    # no bias at final layer\n",
    "        self.outputs  = [ ]  # reset neuron outputs for training\n",
    "        self.deltas   = [ ]  # to be used in backpropagation\n",
    "        pivot         = row.copy( )\n",
    "        \n",
    "        # === Iterate Through the Layers === #\n",
    "        for i in range( len( self.weight ) ):\n",
    "            \n",
    "            # === Add the Bias === #\n",
    "            pivot = np.append( pivot , self.bias[i] )\n",
    "            \n",
    "            # === Dot Product (Weighted Sum) === #\n",
    "            pivot = np.dot( pivot , self.weight[i] )\n",
    "            \n",
    "            # === Activate Neurons === #\n",
    "            pivot = self.functions[ self.activation[i] ]( pivot )\n",
    "            \n",
    "            # === Archive Appropriate Values === #\n",
    "            self.outputs.append( pivot )  # keep record of all activated outputs\n",
    "            self.deltas.append( np.zeros( len( pivot ) ) )  # initializing deltas for later\n",
    "            \n",
    "        # === Return Output of Final Layer === #\n",
    "        return pivot[0]  # MyANN.feed_forward( )\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.back_propagate( row , target )\n",
    "    # ==============================================\n",
    "    def back_propagate( self , row , target ):\n",
    "        \n",
    "        # === Feed Forward to Initialize === #\n",
    "        self.feed_forward( row )\n",
    "        \n",
    "        # === Iterate Backwards Through Layers === #\n",
    "        for i in reversed( range( len( self.weight ) ) ):\n",
    "            \n",
    "            # === Output Layer: No Need for Chain Rule === #\n",
    "            if i == len( self.weight ) - 1:\n",
    "                error = self.compute_loss_gradient( self.outputs[i] , target )\n",
    "                \n",
    "            # === Not Output Layer: Scale Error by Weight (dot) Previous Layer Deltas === #\n",
    "            else:\n",
    "                error = np.dot( self.weight[i+1][:-1] , self.deltas[i+1] )\n",
    "            \n",
    "            # === Use Chain Rule to Determine Current Layer's Deltas === #\n",
    "            self.deltas[i] = error * self.transfer_derivative( self.outputs[i] , self.activation[i] )\n",
    "            \n",
    "        return  # MyANN.back_propagate( )\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.update_weights( row , learning_rate )\n",
    "    # ==============================================\n",
    "    def update_weights( self , row , learning_rate ):\n",
    "        \n",
    "        # === Initialize === #\n",
    "        inputs = row\n",
    "        \n",
    "        # === Iterate Layers === #\n",
    "        for i in range( len( self.weight ) ):\n",
    "            \n",
    "            # === If Hidden Layer, Use Previous Layer's Output as Input === #\n",
    "            if i != 0:\n",
    "                inputs = self.outputs[i-1]\n",
    "                \n",
    "            # === Apply Bias === #\n",
    "            inputs = np.append( inputs , self.bias[i] )\n",
    "            \n",
    "            # === Scale Each Weight Appropriately === #\n",
    "            for j in range( len( inputs ) ):\n",
    "                self.weight[i][j] += learning_rate * self.deltas[i] * inputs[j]\n",
    "                \n",
    "        return  # MyANN.update_weights( )\n",
    "    \n",
    "    \n",
    "    # ==============================================\n",
    "    # MyANN.train( X_train , y_train , epochs , learning_rate )\n",
    "    # ==============================================\n",
    "    def fit( self , X_train , y_train , epochs , learning_rate=None ):\n",
    "        \n",
    "        # === Initialize Learing Rate if Needed === #\n",
    "        if learning_rate is None:\n",
    "            params        = np.sum( [w.shape[0]*w.shape[1] for w in self.weight] )\n",
    "            learning_rate = 1 / ( 2 * params )  # LR is proportionally smaller than num_params\n",
    "            \n",
    "        # === Ensure No Output Layer Bias to Outweight Network === # \n",
    "        self.bias[-1] = 0  # no bias performed at output layer\n",
    "        \n",
    "        # === Each Epoch is 1 Pass Through X_train Rows === #\n",
    "        for e in range( epochs ):\n",
    "            print( 'Epoch {}...'.format( e+1 ) )\n",
    "            \n",
    "            # === Stochastic Gradient Descent: Backpropagate and Update Weights Each Row === #\n",
    "            for i in range( len( X_train ) ):\n",
    "                self.back_propagate( X_train[i] , y_train[i] )\n",
    "                self.update_weights( X_train[i] , learning_rate )\n",
    "               \n",
    "            \n",
    "    # ==============================================\n",
    "    # MyANN.predict( X )\n",
    "    # ==============================================\n",
    "    def predict( self , X ):\n",
    "        \n",
    "        # === Initialization of Output Array === #\n",
    "        output = np.zeros( ( len( X ) , 1 ) )\n",
    "        \n",
    "        # === Perform for Each Row of X === #\n",
    "        for i in range(len(X)):\n",
    "            \n",
    "            # === Feed-Forward Algorithm === #\n",
    "            pivot = X[i].copy( )\n",
    "            for j in range( len( self.weight ) ):\n",
    "                pivot = np.append( pivot , self.bias[j] )\n",
    "                pivot = np.dot( pivot , self.weight[j] )\n",
    "                pivot = self.functions[ self.activation[j] ]( pivot )\n",
    "                \n",
    "            # === Record Results for Output === #\n",
    "            output[i,0] = pivot[0]\n",
    "            \n",
    "        # === Return Final Output === #\n",
    "        return output  # MyANN.predict( )\n",
    "    \n",
    "    def get_layers(self):\n",
    "        #print(len(self.weight))\n",
    "        layers = []\n",
    "        for i in np.arange(len(self.weight)):\n",
    "            #print(self.weight[i].shape)\n",
    "            layers.append(self.weight[i].shape[1])\n",
    "        return layers\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating ANN-1 Without Keras\n",
    "\n",
    "Let's construct `myANN1` object of type `MyANN` defined above. The architecture of myANN1 will have the same structure as the Keras `ann_1` model we constructed before.\n",
    "\n",
    "Also, call the `fit()` method to train on the training dataset for 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a02f10c4b63b32c00891f4a16a8696d4",
     "grade": false,
     "grade_id": "MyANN-1-development",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "myANN1 = MyANN( )\n",
    "myANN1.add_input( X_train_scaled[0].shape[0] )\n",
    "myANN1.add_layer(  5 , 'relu'    )\n",
    "myANN1.add_layer(  4 , 'relu'    )\n",
    "myANN1.add_layer(  1 , 'sigmoid' )\n",
    "\n",
    "''' Now begin the training on the training set (X_train_scaled, y_train)'''\n",
    "myANN1.fit( X_train_scaled , np.array(y_train,dtype=int) , 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94ebdf2fce29ff5cf77f6c0f5ea7a12e",
     "grade": false,
     "grade_id": "utility-myANN-layers",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "An utility function to check number of layers defined in any MyANN object.\n",
    "'''\n",
    "def check_myANN_layers(model, expected_layers):\n",
    "    so_far_so_good = False\n",
    "    if isinstance(model,MyANN) and model.get_layers()==expected_layers:\n",
    "        so_far_so_good = True\n",
    "    return so_far_so_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69fd552817797fc399d405975e736745",
     "grade": true,
     "grade_id": "myANN1-model-check",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Checking if the myANN1 object has the correct architecture.\n",
    "'''\n",
    "assert check_myANN_layers(myANN1,expected_layers=[5,4,1]) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating myANN1 on the Test Set\n",
    "\n",
    "We will now evaluate the test performance of the custom model, `myANN1` on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89fb4a6c7b05b590c9a329a28aa7ada2",
     "grade": false,
     "grade_id": "myANN1-evaluation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Extract Metrics, Archive, and Report === #\n",
    "myann_1_metrics = evaluate_model( myANN1 , X_train_scaled , y_train , X_test_scaled , y_test )\n",
    "model_metrics.append( myann_1_metrics )\n",
    "model_objects.append( myANN1 )\n",
    "myann_1_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing ANN-2 with the custom MyANN class.\n",
    "\n",
    "similar to `myANN1`, we will do the same for `myANN2`, which consists of four relu layers with 8 units each, and a single sigmoid layer for output, similar to `ann_2` (the Keras model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9ccae25df9ad0e0ffab8848c14ca5a1",
     "grade": false,
     "grade_id": "MyANN-2-development",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's construct myANN2, similar to the Keras model, ann_2 that we've constructed before, using MyANN class.\n",
    "Also, call the fit function of myANN2 object and train with training dataset for 25 epochs.\n",
    "'''\n",
    "myANN2 = MyANN( )\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e411cd5017a82a77a513ac17d06b5ef",
     "grade": true,
     "grade_id": "myANN2-model-check",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Checking if the myANN2 object has the correct architecture.\n",
    "'''\n",
    "assert check_myANN_layers(myANN2,expected_layers=[8,8,8,8,1]) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MyANN2\n",
    "\n",
    "We will now evaluate the test performance of the custom model, `myANN2` on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d612ce2c0c05dbf10289f4367fb17f59",
     "grade": false,
     "grade_id": "myANN2-evaluation",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Extract Metrics and Report === #\n",
    "myann_2_metrics = evaluate_model( myANN2 , X_train_scaled , y_train , X_test_scaled , y_test )\n",
    "model_metrics.append( myann_2_metrics )\n",
    "model_objects.append( myANN2 )\n",
    "myann_2_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating ANN-3 Without Keras\n",
    "\n",
    "Finally, using the same class again, we will train a model with the same network structure as the `ann_3` keras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb679e25141f785169e34204479c22c5",
     "grade": false,
     "grade_id": "myANN3-development",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "myANN3 = MyANN( )\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4d8b52677ff53cad0ec30c8cf341dac",
     "grade": true,
     "grade_id": "myANN3-model-check",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "''' Checking if the myANN3 object has the correct architecture.\n",
    "'''\n",
    "assert check_myANN_layers(myANN3,expected_layers=[8,4,2,1]) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MyANN3\n",
    "\n",
    "We will now evaluate the test performance of the custom model, `myANN3` on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract Metrics and Report === #\n",
    "myann_3_metrics = evaluate_model( myANN3 , X_train_scaled , y_train , X_test_scaled , y_test )\n",
    "model_metrics.append( myann_3_metrics )\n",
    "model_objects.append( myANN3 )\n",
    "myann_3_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10 (15 points = 5 + 5 + 5)\n",
    "* Best model, judge and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Best Model\n",
    "\n",
    "Now that we have all the metrics archived for evaluating our models on the test partition of the dataset, we can objectively compare them all and determine which model is objectively the \"best\"\n",
    "\n",
    "For the purposes of this assignment, we will be using the accuracy metric as the objective measure of the \"best\" model. Whichever model is capable of producing the highest accuracy will be packaged and sent to the other submission file to be used on the judgement set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7629165c2f320f2f69848973acdef90a",
     "grade": false,
     "grade_id": "choosing-best-model",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === Initialize Mode/Metrics === #\n",
    "model   = ann_1\n",
    "metrics = ann_1_metrics\n",
    "\n",
    "# === Compare Against all Models to Find the \"Best\" === #\n",
    "for i in range( len( model_metrics ) ):\n",
    "    if metrics['accuracy'] < model_metrics[i]['accuracy']:\n",
    "        model   = model_objects[i]\n",
    "        metrics = model_metrics[i]\n",
    "\n",
    "# === Report Accuracy of \"Best\" Model === #\n",
    "print( 'Best Accuracy: {}'.format( metrics['accuracy'] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Objects for Use Against Judgement\n",
    "\n",
    "Now that we have determined the best model with respect to test accuracy, we will  --\n",
    "\n",
    "* save the model object along with the encoders, standard scaler (i.e., normalizer) that was fit in the preprocessing step. That way we can encode and scale any test/judgement dataset later according to the same metrics as we did the other datasets without reloading/re-fitting/re-training the model. \n",
    "\n",
    "* In order to save these objects, we will use python's `pickle` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea1184f052a334bd1bba21ef7b00728c",
     "grade": false,
     "grade_id": "saving-objects",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# === If MyANN Model is Best, Pickle the Model === #\n",
    "if isinstance( model , MyANN ):\n",
    "    dump( model , open( 'dataset/model.sav' , 'wb' ) )\n",
    "    # === Otherwise, Keras model.save( ) === #\n",
    "else:\n",
    "    model.save( 'dataset/model.sav', save_format='h5' )\n",
    "    \n",
    "# === Pickle to Save the OneHotEncoders=== #\n",
    "dump( [enc_list,categorical_features], open( 'dataset/onehotencoder.pkl' , 'wb' ) )\n",
    "    \n",
    "# === Pickle to Save the Normalizer === #\n",
    "dump( normalizer , open( 'dataset/normalizer.pkl' , 'wb' ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dde0f5d190c24ccc4634855304aaea60",
     "grade": true,
     "grade_id": "check-saved-objects",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Check if you have saved the objects mentioned above.\n",
    "'''\n",
    "model_file = Path(\"dataset/model.sav\")\n",
    "onehotencoder_file = Path(\"dataset/onehotencoder.pkl\")\n",
    "normalizer_file = Path(\"dataset/normalizer.pkl\")\n",
    "\n",
    "assert model_file.is_file() and onehotencoder_file.is_file() and normalizer_file.is_file() == True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the judge.csv based evaluation\n",
    "The following cells contain documented judgement code for the assignment, as specified by the assignment instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "In the training file, some preprocessing was performed to the dataset prior to training the model. \n",
    "\n",
    "In order to ensure the model (i.e., the best model) predicts the output appropriately, it must be preprocessed similarly to the training methods. \n",
    "\n",
    "The following cells perform these transformations on the judgement set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b739ab3974a7a6e55be4c884874c54c7",
     "grade": false,
     "grade_id": "load-judge-csv",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reading the judge.csv dataset... it's similar to test set, without the target column.\n",
    "This is the same scenario you would encounter in real-world problems... \n",
    "    * You are given dataset with targets labels... you build a model... you do test evaluations...\n",
    "        you pick your best model ... and then you are out in the wild... \n",
    "            wanting to predict real/unknown samples... without prior information.\n",
    "'''\n",
    "# === Use the Above Function and Demonstrate Success === #\n",
    "judge_dataset = read_dataset( 'dataset/judge.csv' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the following three preprocessings on the judge_dataset:\n",
    "1. First preprocessing is Dropping  Features: {CustomerID, Surname}.\n",
    "2. Fifth preprocessing is to One hot encoding based on the judge_set\n",
    "    * Make sure you use the onehot-encoders you've saved before.\n",
    "3. Sixth preprocessing is Normalization\n",
    "    * Make sure you use the normalizer you've saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e779f9ce04a7de313deea4dc24247c9",
     "grade": false,
     "grade_id": "preprocessing-judge-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. First preprocessing is dropping 'CustomerID' and 'Surname' columns from judge_dataset. Save the resulting\n",
    "    dataset as judge_first.\n",
    "2. Fifth preprocessing is one-hot encoding of the judge_first dataset using the same encoders on the \n",
    "    same categorical features you've encoded the training set. Save the resulting dataset as judge_fifth.\n",
    "3. Sixth preprocessing is normalizing the judge_fifth dataset based on the the same normalizer you applied \n",
    "    on training/test set. Save the resulting dataset as judge_sixth.\n",
    "'''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f5957e90fd854a6deafc76f519efb1e",
     "grade": true,
     "grade_id": "check-judge-preprocess",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Checking if the 3 judge dataset preprocessing are done correctly. I'm here checking only one-hot encoding.\n",
    "'''\n",
    "assert  check_one_hot_encoding(X_train_ohe,judge_fifth) == True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Best Model to predict the judge samples\n",
    "\n",
    "In the training notebook, we saved our best performing model as `model.sav`. \n",
    "\n",
    "Please note, `model.sav` file could be either a `MyANN` object, or `Keras` model. Loading function to these\n",
    "two types are different. So, let's first try with `pickle.load`, and if it throws an exception, then\n",
    "we will try to load it via `Keras` model load function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "037d8ca5048ae25227a2e9b2d7ec6f48",
     "grade": false,
     "grade_id": "loading-best-model",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model = load(open('dataset/model.sav','rb'))\n",
    "except:\n",
    "    model = keras.models.load_model( 'dataset/model.sav' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (5 points)\n",
    "### Predict 1 and 0 for all the samples in the judge_sixth dataset.\n",
    "\n",
    "Please note, your prediction results should be either 0 or 1. Fractions are not allowed.\n",
    "\n",
    "Also, prepare a pandas dataframe, judge_pred_df containing 2 columns: {CustomerId, Exited}.\n",
    "The dataframe will contain CustomerId and corresponding predicted 0/1 values according to your best model evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50e468c2c73cc529ebe405bff10d76e7",
     "grade": false,
     "grade_id": "judge_evaluation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare prediction of the judge_sixth preprocessed set using the best model.\n",
    "Save your prediction results in judge_pred variable.\n",
    "Make sure the judge_pred contains only 0 and 1. No fractions will be allowed.\n",
    "Also, make sure number of items in judge_pred matches the number of samples in the judge set.\n",
    "'''\n",
    "judge_pred = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f779c13f95f759d30324c1395a03e469",
     "grade": false,
     "grade_id": "saving_judge_predictions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Saving the judge_pred_df dataframe into 'dataset/judge_pred_df.csv' file.\n",
    "'''\n",
    "judge_pred_df.to_csv( 'dataset/judge_pred_df.csv' , index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e8dbc53e12ffea08e4fc512c87a13ee",
     "grade": true,
     "grade_id": "judge_pred_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Checking your judge predictions to put some score.\n",
    "\n",
    "\n",
    "'''\n",
    "#csv_student_file = open(\"dataset/judge_pred_df.csv\", \"rb\")\n",
    "#test_response = requests.post(\"http://54.160.44.72/csci5931sp22/check-PA1-judge-pred.php\", \n",
    "#                              files = {\"judge_pred\": csv_student_file})\n",
    "files = {'judge_pred': open('dataset/judge_pred_df.csv', 'rb')}\n",
    "\n",
    "\n",
    "response = requests.post(\n",
    "                       url='http://54.160.44.72/csci5931sp22/check-PA1-judge-pred.php', \n",
    "                       files=files,\n",
    "                        data = {'submit':'submit'})\n",
    "\n",
    "\n",
    "assert response.ok == True\n",
    "print(response.text)\n",
    "\n",
    "#    print(\"Upload completed successfully!\")\n",
    "#    print(test_response.text)\n",
    "#else:\n",
    "#    print(\"Something went wrong!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all. Thanks for your work... :)\n",
    "\n",
    "Now, do the following to earn credit --\n",
    "1. please hit the 'Save' button, or go \"File > Save and Checkpoint\" menu option to save the notebook.\n",
    "2. Submit this notebook \"PA1-assignment.ipynb\" into Canvas \"Assignment 1\" entry. \n",
    "3. Done! You will receive a grade in the next 2-3 academic days.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
