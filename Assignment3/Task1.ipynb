{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef20138",
   "metadata": {},
   "source": [
    "Task 1 : Character level Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066af548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6837946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the 3 text files\n",
    "\n",
    "file = open(\"Text1.txt\",\"r\")\n",
    "if(file.mode == \"r\"):\n",
    "    text1 = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"Text2.txt\",\"r\")\n",
    "if(file.mode == \"r\"):\n",
    "    text2 = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"Text3.txt\",\"r\")\n",
    "if(file.mode == \"r\"):\n",
    "    text3 = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the 3 texts\n",
    "data = text1+text2+text3\n",
    "\n",
    "#Converting upper case to lower case\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654c9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building char_to_index and index_to_char vectors.\n",
    "\n",
    "characters = list(set(data))\n",
    "char_to_index = {ch:i for i,ch in enumerate(characters)}\n",
    "index_to_char = {i:ch for i,ch in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00403454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into characters\n",
    "\n",
    "def split(word): \n",
    "    return [char for char in word]\n",
    "\n",
    "data = split(data)\n",
    "\n",
    "#Changing all the characters into indices using the char_to_index vector\n",
    "\n",
    "for ch in range(len(data)):\n",
    "    data[ch] = char_to_index[data[ch]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e26f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the entire data into lists of 20 characters each.\n",
    "\n",
    "x = []\n",
    "c = []\n",
    "for i in range(len(data)):\n",
    "    if i%20==0:\n",
    "        x.append(c)\n",
    "        c = []\n",
    "    else:\n",
    "        c.append(data[i])\n",
    "x.remove(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf9ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the entire data in lists of 20 characters each and create the training data y.\n",
    "\n",
    "last_char = data[0] \n",
    "data.remove(data[0])\n",
    "\n",
    "y = []\n",
    "c =[]\n",
    "for i in range(len(data)):\n",
    "    if i%20==0:\n",
    "        y.append(c)\n",
    "        c = []\n",
    "    else:\n",
    "        c.append(data[i])\n",
    "        \n",
    "y.remove(y[0])\n",
    "y[-1].append(last_char)\n",
    "data.append(last_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19490786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the vocabulary size as the number of unique characters in the data.\n",
    "\n",
    "vocabulary_size = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75533181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNVanilla:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=10):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim   #size of the vocabulary\n",
    "        self.hidden_dim = hidden_dim  # size of hidden layer\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0fac0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b500f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "        \n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "        \n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "        \n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "        \n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]  #We not only return the calculated outputs, but also the hidden states. \n",
    "                   #We will use them later to calculate the gradients\n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7706ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0853214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    \n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        \n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        \n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    " \n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    " \n",
    "RNNVanilla.calculate_total_loss = calculate_total_loss\n",
    "RNNVanilla.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d9decb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        \n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        \n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            \n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            \n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNVanilla.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c81d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    \n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    \n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print (\"+h Loss: %f\" % gradplus)\n",
    "                print (\"-h Loss: %f\" % gradminus)\n",
    "                print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print (\"Relative Error: %f\" % relative_error)\n",
    "                return\n",
    "            it.iternext()\n",
    "        print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    " \n",
    "RNNVanilla.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aabb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_13289/3102754607.py:30: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e48076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    \n",
    "RNNVanilla.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce416b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.01, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e66de166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-07 14:32:23: Loss after num_examples_seen=0 epoch=0: 4.324536\n",
      "2022-04-07 14:32:41: Loss after num_examples_seen=2000 epoch=4: 2.474720\n",
      "2022-04-07 14:32:57: Loss after num_examples_seen=4000 epoch=8: 2.378042\n",
      "2022-04-07 14:33:16: Loss after num_examples_seen=6000 epoch=12: 2.295962\n",
      "2022-04-07 14:33:36: Loss after num_examples_seen=8000 epoch=16: 2.187207\n"
     ]
    }
   ],
   "source": [
    "# Train our rnn model on a small subset of the data \n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(vocabulary_size)\n",
    "losses = train_with_sgd(model, x[:500], y[:500], nepoch=20, evaluate_loss_after=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82b9ca28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a graph between the loss and number of epochs\n",
    "\n",
    "losses_array = np.asarray(losses)\n",
    "losses_array\n",
    "num_of_epochs = []\n",
    "loss_per_epoch = []\n",
    "for i in range(losses_array.shape[0]):\n",
    "    num_of_epochs.append((losses_array[i][0]/100))\n",
    "    loss_per_epoch.append(losses_array[i][1])\n",
    "    \n",
    "plt.plot(num_of_epochs,loss_per_epoch,marker ='o', markerfacecolor = 'red')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5924aaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   ld yhe allessiwath\n",
      "in callornm ua taly misinstimhidibe cal[ounicalloors a allathe wave iaelifllorov\n",
      "2   n weth\n",
      "]o the atrealitarcilleocc, inlliog at collicling to a ollyts liighe eamlint resèilionp. in o\n",
      "3   ry, itha hinling alighto the billoon ia acloegenazh in intlina\n",
      "o the bighins.0the li20 a\n",
      "bucacap al\n",
      "4   nd aosto:s calleavevaiont ost arheasevlist istaeancamire bith\n",
      "ohovelamkaacallathesig if lalkteliinc\n",
      "5   troangouith ity a apaeaweiac. inth atsivloint illfeltkinto a\n",
      "dathesuebcest rapevaideagale ald.gadpe\n",
      "6   nd iot, alafrouiatiwerot loyepauneet, a, e0selia to the ing!anseyratainlamo afreich.e bapaicesminil\n",
      "7   lloo the kelith, suseasishe in aslangericat ovemallurollay, hathaanded apesule\n",
      "balloonte nat. in ai\n",
      "8   nged hidesevallomatala\n",
      "tallelauc, at apanion? te lialioain wa ftougaiane of the, ntoraalweluvens th\n",
      "9   gitf oll a a iullalloone siteant ailio the folle irethespanathescaso a a fald, avelat\n",
      "wa la2n alift\n",
      "10   s aaleem kuin. the bild on the calllusteraghivean afe, atroingloon astibagaillone muss afreathe hai\n"
     ]
    }
   ],
   "source": [
    "# generating text using the model \n",
    "\n",
    "num_sentences = 10 # I am generating 10 sentences\n",
    "sentence_max_length = 100 # each sentence has max length of 100 characters \n",
    "\n",
    "def generate_sentence(model,sentence_max_length):\n",
    "    # We start the sentence with a random character here I have chosen 'a'\n",
    "    new_sentence = [char_to_index['a']]\n",
    "    i = 0\n",
    "    \n",
    "    # Repeat until we get a sentence of desired length\n",
    "    while i<sentence_max_length:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        next_word = next_word_probs[0]\n",
    "        samples = np.random.multinomial(1, next_word[-1])\n",
    "        sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        i = i+1\n",
    "    sentence_str = [index_to_char[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    sent = generate_sentence(model,sentence_max_length)\n",
    "    full = (\"\".join(sent))\n",
    "    print(i+1,\" \",full)\n",
    "    \n",
    "# generating 10 sentences with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1f6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
