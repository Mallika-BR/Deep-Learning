{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b722e52",
   "metadata": {},
   "source": [
    "Task 3: Word-level Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdae595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6de00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the 3 text files \n",
    "\n",
    "file = open(\"Text1.txt\",\"r\")\n",
    "if(file.mode == \"r\"):\n",
    "    text1 = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"Text2.txt\",\"r\")\n",
    "if(file.mode == \"r\"):\n",
    "    text2 = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"Text3.txt\",\"r\")\n",
    "if(file.mode == \"r\"):\n",
    "    text3 = file.read()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b783e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the 3 texts\n",
    "data = text1+text2+text3\n",
    "\n",
    "#Converting upper case to lower case\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2beabaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into sentences\n",
    "sentences = nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5df6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the values of vocabulary and other parameters \n",
    "vocabulary_size = 10000\n",
    "unknown_token = \"UNKNOWN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44221c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 19402 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Append SENTENCE_START and SENTENCE_END\n",
    "\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print( \"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1b5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the special characters\n",
    "\n",
    "for x in range(len(sentences)):\n",
    "    sentences[x] = sentences[x].replace(\"\\n\",\" \")\n",
    "    sentences[x] = sentences[x].replace(\"*\",\"\")\n",
    "    sentences[x] = re.sub(r'[^a-zA-Z0-9_\\s]+', '', sentences[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7bb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa07c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16317 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac9414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba8d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff73b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_14351/1408238981.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  XTrain = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_14351/1408238981.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  yTrain = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "XTrain = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "yTrain = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e698eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating class for RNN vanilla model\n",
    "class RNNVanilla:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=10):\n",
    "        \n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim   #size of the vocabulary\n",
    "        self.hidden_dim = hidden_dim  # size of hidden layer\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "# function for softmax activation\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "# function for forward propagation\n",
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "        \n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "        \n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "        \n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "        \n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]  #We not only return the calculated outputs, but also the hidden states. \n",
    "                   #We will use them later to calculate the gradients\n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.forward_propagation = forward_propagation\n",
    "\n",
    "# function to predict next word\n",
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "#Now make it a member of the class RNNVanilla\n",
    "RNNVanilla.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2317f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total error \n",
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    \n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        \n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        \n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    " \n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    " \n",
    "RNNVanilla.calculate_total_loss = calculate_total_loss\n",
    "RNNVanilla.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fff61e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for back propagation through time for updating gradients\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        \n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        \n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            \n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            \n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNVanilla.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae54b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradients using backpropagation.\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    \n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    \n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print (\"+h Loss: %f\" % gradplus)\n",
    "                print (\"-h Loss: %f\" % gradminus)\n",
    "                print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print (\"Relative Error: %f\" % relative_error)\n",
    "                return\n",
    "            it.iternext()\n",
    "        print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    " \n",
    "RNNVanilla.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0b96878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_14351/2632208630.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1748ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    \n",
    "RNNVanilla.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfc67870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.01, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "404d92ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 12:42:24: Loss after num_examples_seen=0 epoch=0: 9.210322\n",
      "2022-04-21 12:45:38: Loss after num_examples_seen=2000 epoch=4: 6.040192\n",
      "2022-04-21 12:49:02: Loss after num_examples_seen=4000 epoch=8: 5.773455\n",
      "2022-04-21 12:52:27: Loss after num_examples_seen=6000 epoch=12: 5.597584\n",
      "2022-04-21 12:55:54: Loss after num_examples_seen=8000 epoch=16: 5.660205\n",
      "Setting learning rate to 0.005000\n"
     ]
    }
   ],
   "source": [
    "# Train on a small subset of the data to see what happens\n",
    "np.random.seed(10)\n",
    "model = RNNVanilla(vocabulary_size)\n",
    "losses = train_with_sgd(model, XTrain[:500], yTrain[:500], nepoch=20, evaluate_loss_after=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9edd90c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a graph between the loss and number of epochs\n",
    "\n",
    "losses_array = np.asarray(losses)\n",
    "\n",
    "num_of_epochs = []\n",
    "loss_per_epoch = []\n",
    "for i in range(losses_array.shape[0]):\n",
    "    num_of_epochs.append((losses_array[i][0])/100)\n",
    "    loss_per_epoch.append(losses_array[i][1])\n",
    "    \n",
    "plt.plot(num_of_epochs,loss_per_epoch,marker ='o', markerfacecolor = 'red')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fc770dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   the dog the persons was a furious\n",
      "1   pressing he was besiegers to without say great 2010 no for the shore of it of the 1865 of the duty and a shellfish of a practice gibraltar his together in the cliff spirit is as\n",
      "2   it was will but the shore but these they gutenberg the gas by the stranger was they many hemisphere he we than the reuse is stream they had be\n",
      "3   gideon the one to the ocean set not them from the sea\n",
      "4   the sailor and seen and a subterranean hundred fire be be for we smallest on the passage of the left the opposite\n",
      "5   it was not for the disposal then breadth\n",
      "6   rain was the granite pair of the midst in their plunged and some for prepare they wall the ropes\n",
      "7   it appeared this appeared so the midst of march up hoisted and to a quarter comfortable herald directly and no heart we this uncovered day webfooted by a 26th and slippery to this being abounded men it spray off at provisionally life they had the sea\n",
      "8   he would want for a use in the donation gutenberg of the first and they might\n",
      "9   a evening then food im by this thirtyfifth and the are was 1\n"
     ]
    }
   ],
   "source": [
    "# generate text using the model\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    i = 0\n",
    "    \n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        next_word = next_word_probs[0]\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        i = i+1\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    full = (\" \".join(sent))\n",
    "    full = full.replace(\"SENTENCE_END\",\".\")\n",
    "    full = full.replace(\"SENTENCE_START\",\"\")\n",
    "    print(i,\" \",full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dcc75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
